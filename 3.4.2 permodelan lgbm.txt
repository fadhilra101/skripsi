sekarang kita menuju 3.4.2 permodelan LGBM:
1. arsitektur model lightgbm
LightGBM, which contains two novel techniques: Gradient-based One-Side Sampling and Exclusive Feature Bundling to deal with large number of data instances and large number of features respectively. We have performed both theoretical analysis and experimental studies on these two techniques. GOSS keeps all the instances with large gradients and performs random sampling on the instances with small gradients. In order to compensate the influence to the data distribution,when computing the information gain, GOSS introduces a constant multiplier for the data instances with small gradients. EFB algorithm can bundle many exclusive features to the much fewer dense features, which can effectively avoid unnecessary computation for zero feature values. Actually, we can also optimize the basic histogram-based algorithm towards ignoring the zero feature values by using a table for each feature to record the data with nonzero values. Ke, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., ... & Liu, T. Y. (2017). Lightgbm: A highly efficient gradient boosting decision tree. Advances in neural information processing systems, 30.

buat teks paragraf nya yang baik dengan format yang semirip mungkin dengan teks berikut:
Pada penelitian ini metode yang digunakan adalah LSTM. Arsitekrut model terdiri atas lima layer seperti yang ditunjukan pada Gambar 3.6. Lima layer tersebut meliputi embedding layer, LSTM layer, dense layer (fully connected layer), dropout layer, dan dense layer (output layer). Embedding layer digunakan untuk memproyeksikan setiap kata dalam kamus kata yang dipakai dalam pelatihan model ke dalam dimensi yang telah ditentukan yaitu 256 dimensi. Input pada embedding layer berupa dokumen dengan panjang maksimal sebesar 200 dan output nya adalah sebuah urutan vektor dengan dimensi 256 sehingga memiliki bentuk (200, 256).  Output dari embedding layer yang berupa urutan vektor dengan bentuk (200, 256) akan diteruskan ke dalam LSTM layer untuk mengenali pola urutan dalam data. Jumlah neuron yang digunakan pada LSTM layer untuk memproses Pada penelitian ini metode yang digunakan adalah LSTM. Arsitekrut model terdiri atas lima layer seperti yang ditunjukan pada Gambar 3.6. Lima layer tersebut meliputi embedding layer, LSTM layer, dense layer (fully connected layer), dropout layer, dan dense layer (output layer). Embedding layer digunakan untuk memproyeksikan setiap kata dalam kamus kata yang dipakai dalam pelatihan model ke dalam dimensi yang telah ditentukan yaitu 256 dimensi. Input pada embedding layer berupa dokumen dengan panjang maksimal sebesar 200 dan output nya adalah sebuah urutan vektor dengan dimensi 256 sehingga memiliki bentuk (200, 256).  Output dari embedding layer yang berupa urutan vektor dengan bentuk (200, 256) akan diteruskan ke dalam LSTM layer untuk mengenali pola urutan dalam data. Jumlah neuron yang digunakan pada LSTM layer untuk memproses 